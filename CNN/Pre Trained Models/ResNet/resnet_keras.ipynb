{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94112989",
   "metadata": {},
   "source": [
    "# ***KERAS PRE TRAINED MODELS***\n",
    "https://keras.io/api/applications/\n",
    "<table border=\"1\" cellspacing=\"0\" cellpadding=\"5\">\n",
    "  <thead>\n",
    "    <tr>\n",
    "      <th>Available models</th>\n",
    "      <th>Size (MB)</th>\n",
    "      <th>Top-1 Accuracy</th>\n",
    "      <th>Top-5 Accuracy</th>\n",
    "      <th>Parameters</th>\n",
    "      <th>Depth</th>\n",
    "      <th>Time (ms) per inference step (CPU)</th>\n",
    "      <th>Time (ms) per inference step (GPU)</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr><td>Xception</td><td>88</td><td>79.0%</td><td>94.5%</td><td>22.9M</td><td>81</td><td>109.4</td><td>8.1</td></tr>\n",
    "    <tr><td>VGG16</td><td>528</td><td>71.3%</td><td>90.1%</td><td>138.4M</td><td>16</td><td>69.5</td><td>4.2</td></tr>\n",
    "    <tr><td>VGG19</td><td>549</td><td>71.3%</td><td>90.0%</td><td>143.7M</td><td>19</td><td>84.8</td><td>4.4</td></tr>\n",
    "    <tr><td>ResNet50</td><td>98</td><td>74.9%</td><td>92.1%</td><td>25.6M</td><td>107</td><td>58.2</td><td>4.6</td></tr>\n",
    "    <tr><td>ResNet50V2</td><td>98</td><td>76.0%</td><td>93.0%</td><td>25.6M</td><td>103</td><td>45.6</td><td>4.4</td></tr>\n",
    "    <tr><td>ResNet101</td><td>171</td><td>76.4%</td><td>92.8%</td><td>44.7M</td><td>209</td><td>89.6</td><td>5.2</td></tr>\n",
    "    <tr><td>ResNet101V2</td><td>171</td><td>77.2%</td><td>93.8%</td><td>44.7M</td><td>205</td><td>72.7</td><td>5.4</td></tr>\n",
    "    <tr><td>ResNet152</td><td>232</td><td>76.6%</td><td>93.1%</td><td>60.4M</td><td>311</td><td>127.4</td><td>6.5</td></tr>\n",
    "    <tr><td>ResNet152V2</td><td>232</td><td>78.0%</td><td>94.2%</td><td>60.4M</td><td>307</td><td>107.5</td><td>6.6</td></tr>\n",
    "    <tr><td>InceptionV3</td><td>92</td><td>77.9%</td><td>93.7%</td><td>23.9M</td><td>189</td><td>42.2</td><td>6.9</td></tr>\n",
    "    <tr><td>InceptionResNetV2</td><td>215</td><td>80.3%</td><td>95.3%</td><td>55.9M</td><td>449</td><td>130.2</td><td>10.0</td></tr>\n",
    "    <tr><td>MobileNet</td><td>16</td><td>70.4%</td><td>89.5%</td><td>4.3M</td><td>55</td><td>22.6</td><td>3.4</td></tr>\n",
    "    <tr><td>MobileNetV2</td><td>14</td><td>71.3%</td><td>90.1%</td><td>3.5M</td><td>105</td><td>25.9</td><td>3.8</td></tr>\n",
    "    <tr><td>DenseNet121</td><td>33</td><td>75.0%</td><td>92.3%</td><td>8.1M</td><td>242</td><td>77.1</td><td>5.4</td></tr>\n",
    "    <tr><td>DenseNet169</td><td>57</td><td>76.2%</td><td>93.2%</td><td>14.3M</td><td>338</td><td>96.4</td><td>6.3</td></tr>\n",
    "    <tr><td>DenseNet201</td><td>80</td><td>77.3%</td><td>93.6%</td><td>20.2M</td><td>402</td><td>127.2</td><td>6.7</td></tr>\n",
    "    <tr><td>NASNetMobile</td><td>23</td><td>74.4%</td><td>91.9%</td><td>5.3M</td><td>389</td><td>27.0</td><td>6.7</td></tr>\n",
    "    <tr><td>NASNetLarge</td><td>343</td><td>82.5%</td><td>96.0%</td><td>88.9M</td><td>533</td><td>344.5</td><td>20.0</td></tr>\n",
    "    <tr><td>EfficientNetB0</td><td>29</td><td>77.1%</td><td>93.3%</td><td>5.3M</td><td>132</td><td>46.0</td><td>4.9</td></tr>\n",
    "    <tr><td>EfficientNetB1</td><td>31</td><td>79.1%</td><td>94.4%</td><td>7.9M</td><td>186</td><td>60.2</td><td>5.6</td></tr>\n",
    "    <tr><td>EfficientNetB2</td><td>36</td><td>80.1%</td><td>94.9%</td><td>9.2M</td><td>186</td><td>80.8</td><td>6.5</td></tr>\n",
    "    <tr><td>EfficientNetB3</td><td>48</td><td>81.6%</td><td>95.7%</td><td>12.3M</td><td>210</td><td>140.0</td><td>8.8</td></tr>\n",
    "    <tr><td>EfficientNetB4</td><td>75</td><td>82.9%</td><td>96.4%</td><td>19.5M</td><td>258</td><td>308.3</td><td>15.1</td></tr>\n",
    "    <tr><td>EfficientNetB5</td><td>118</td><td>83.6%</td><td>96.7%</td><td>30.6M</td><td>312</td><td>579.2</td><td>25.3</td></tr>\n",
    "    <tr><td>EfficientNetB6</td><td>166</td><td>84.0%</td><td>96.8%</td><td>43.3M</td><td>360</td><td>958.1</td><td>40.4</td></tr>\n",
    "    <tr><td>EfficientNetB7</td><td>256</td><td>84.3%</td><td>97.0%</td><td>66.7M</td><td>438</td><td>1578.9</td><td>61.6</td></tr>\n",
    "    <tr><td>EfficientNetV2B0</td><td>29</td><td>78.7%</td><td>94.3%</td><td>7.2M</td><td>-</td><td>-</td><td>-</td></tr>\n",
    "    <tr><td>EfficientNetV2B1</td><td>34</td><td>79.8%</td><td>95.0%</td><td>8.2M</td><td>-</td><td>-</td><td>-</td></tr>\n",
    "    <tr><td>EfficientNetV2B2</td><td>42</td><td>80.5%</td><td>95.1%</td><td>10.2M</td><td>-</td><td>-</td><td>-</td></tr>\n",
    "    <tr><td>EfficientNetV2B3</td><td>59</td><td>82.0%</td><td>95.8%</td><td>14.5M</td><td>-</td><td>-</td><td>-</td></tr>\n",
    "    <tr><td>EfficientNetV2S</td><td>88</td><td>83.9%</td><td>96.7%</td><td>21.6M</td><td>-</td><td>-</td><td>-</td></tr>\n",
    "    <tr><td>EfficientNetV2M</td><td>220</td><td>85.3%</td><td>97.4%</td><td>54.4M</td><td>-</td><td>-</td><td>-</td></tr>\n",
    "    <tr><td>EfficientNetV2L</td><td>479</td><td>85.7%</td><td>97.5%</td><td>119.0M</td><td>-</td><td>-</td><td>-</td></tr>\n",
    "    <tr><td>ConvNeXtTiny</td><td>109.42</td><td>81.3%</td><td>-</td><td>28.6M</td><td>-</td><td>-</td><td>-</td></tr>\n",
    "    <tr><td>ConvNeXtSmall</td><td>192.29</td><td>82.3%</td><td>-</td><td>50.2M</td><td>-</td><td>-</td><td>-</td></tr>\n",
    "    <tr><td>ConvNeXtBase</td><td>338.58</td><td>85.3%</td><td>-</td><td>88.5M</td><td>-</td><td>-</td><td>-</td></tr>\n",
    "    <tr><td>ConvNeXtLarge</td><td>755.07</td><td>86.3%</td><td>-</td><td>197.7M</td><td>-</td><td>-</td><td>-</td></tr>\n",
    "    <tr><td>ConvNeXtXLarge</td><td>1310</td><td>86.7%</td><td>-</td><td>350.1M</td><td>-</td><td>-</td><td>-</td></tr>\n",
    "  </tbody>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de32541a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.resnet import ResNet50\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input, decode_predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f474a9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels.h5\n",
      "\u001b[1m102967424/102967424\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 0us/step\n"
     ]
    }
   ],
   "source": [
    "model = ResNet50(weights='imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b11c2573",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "img_path = 'car.jpg'\n",
    "img =  image.load_img(img_path, target_size=(224, 224))\n",
    "x = image.img_to_array(img)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "x = preprocess_input(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b93d2e5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step\n",
      "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/imagenet_class_index.json\n",
      "\u001b[1m35363/35363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2us/step\n",
      "prediction [('n04285008', 'sports_car', np.float32(0.36901015)), ('n03459775', 'grille', np.float32(0.3368053)), ('n03100240', 'convertible', np.float32(0.15836184))]\n"
     ]
    }
   ],
   "source": [
    "preds = model.predict(x)\n",
    "print('prediction', decode_predictions(preds, top=3)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ec9e3c8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step\n",
      "prediction [('n07747607', 'orange', np.float32(0.9365506)), ('n07749582', 'lemon', np.float32(0.06342674)), ('n07753592', 'banana', np.float32(5.8262012e-06))]\n"
     ]
    }
   ],
   "source": [
    "img_path = 'orange.jpg'\n",
    "img =  image.load_img(img_path, target_size=(224, 224))\n",
    "x = image.img_to_array(img)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "x = preprocess_input(x)\n",
    "preds = model.predict(x)\n",
    "print('prediction', decode_predictions(preds, top=3)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dcae787b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148ms/step\n",
      "prediction [('n04120489', 'running_shoe', np.float32(0.91191345)), ('n03047690', 'clog', np.float32(0.023362821)), ('n04133789', 'sandal', np.float32(0.021984523))]\n"
     ]
    }
   ],
   "source": [
    "img_path = 'shoe.jpg'\n",
    "img =  image.load_img(img_path, target_size=(224, 224))\n",
    "x = image.img_to_array(img)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "x = preprocess_input(x)\n",
    "preds = model.predict(x)\n",
    "print('prediction', decode_predictions(preds, top=3)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ffd98384",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step\n",
      "prediction [('n02708093', 'analog_clock', np.float32(0.4908206)), ('n04328186', 'stopwatch', np.float32(0.28166562)), ('n04548280', 'wall_clock', np.float32(0.15705863))]\n"
     ]
    }
   ],
   "source": [
    "img_path = 'watch.jpg'\n",
    "img =  image.load_img(img_path, target_size=(224, 224))\n",
    "x = image.img_to_array(img)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "x = preprocess_input(x)\n",
    "preds = model.predict(x)\n",
    "print('prediction', decode_predictions(preds, top=3)[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
