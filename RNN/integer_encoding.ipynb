{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5447b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\n",
    "    \"The cat sat on the mat.\",\n",
    "    \"The dog barked loudly at night.\",\n",
    "    \"I love eating apples and bananas.\",\n",
    "    \"She is reading a book in the library.\",\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"Machine learning is a branch of artificial intelligence.\",\n",
    "    \"Deep learning models need a lot of data.\",\n",
    "    \"The weather today is sunny and bright.\",\n",
    "    \"Natural language processing helps computers understand text.\",\n",
    "    \"He bought fresh vegetables from the market.\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79834c0",
   "metadata": {},
   "source": [
    "# What Tokenizer Does\n",
    "\n",
    "1. Builds a Vocabulary\n",
    "\n",
    "    - It scans all the training texts and assigns each unique word an integer index.\n",
    "\n",
    "    INPUT ---> ` texts = [\"I love cats\", \"I love dogs\"] `\n",
    "    \n",
    "    OUTPUT ---> ` {'i': 1, 'love': 2, 'cats': 3, 'dogs': 4} `\n",
    "\n",
    "2. Converts Text → Sequences of Integers\n",
    "\n",
    "    - After fitting, you can transform sentences into lists of integers.\n",
    "\n",
    "    INPUT ---> ` tokenizer.texts_to_sequences([\"I love dogs\"]) `\n",
    "    \n",
    "    OUTPUT ---> ` [[1, 2, 4]] `\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90236033",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4f4e33",
   "metadata": {},
   "source": [
    "### Problem\n",
    "\n",
    "When you fit a Tokenizer on your training text, it builds a vocabulary (word → integer mapping).\n",
    "\n",
    "But during inference (testing or production), you might encounter a new word that was not in the training vocabulary.\n",
    "\n",
    "### Solution: oov_token\n",
    "\n",
    "If you set an OOV token, every unknown word will be replaced with a special token (like \"nothing\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37857cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(oov_token='<nothing>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "691b2f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "26377cf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<nothing>': 1,\n",
       " 'the': 2,\n",
       " 'is': 3,\n",
       " 'a': 4,\n",
       " 'dog': 5,\n",
       " 'and': 6,\n",
       " 'learning': 7,\n",
       " 'of': 8,\n",
       " 'cat': 9,\n",
       " 'sat': 10,\n",
       " 'on': 11,\n",
       " 'mat': 12,\n",
       " 'barked': 13,\n",
       " 'loudly': 14,\n",
       " 'at': 15,\n",
       " 'night': 16,\n",
       " 'i': 17,\n",
       " 'love': 18,\n",
       " 'eating': 19,\n",
       " 'apples': 20,\n",
       " 'bananas': 21,\n",
       " 'she': 22,\n",
       " 'reading': 23,\n",
       " 'book': 24,\n",
       " 'in': 25,\n",
       " 'library': 26,\n",
       " 'quick': 27,\n",
       " 'brown': 28,\n",
       " 'fox': 29,\n",
       " 'jumps': 30,\n",
       " 'over': 31,\n",
       " 'lazy': 32,\n",
       " 'machine': 33,\n",
       " 'branch': 34,\n",
       " 'artificial': 35,\n",
       " 'intelligence': 36,\n",
       " 'deep': 37,\n",
       " 'models': 38,\n",
       " 'need': 39,\n",
       " 'lot': 40,\n",
       " 'data': 41,\n",
       " 'weather': 42,\n",
       " 'today': 43,\n",
       " 'sunny': 44,\n",
       " 'bright': 45,\n",
       " 'natural': 46,\n",
       " 'language': 47,\n",
       " 'processing': 48,\n",
       " 'helps': 49,\n",
       " 'computers': 50,\n",
       " 'understand': 51,\n",
       " 'text': 52,\n",
       " 'he': 53,\n",
       " 'bought': 54,\n",
       " 'fresh': 55,\n",
       " 'vegetables': 56,\n",
       " 'from': 57,\n",
       " 'market': 58}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cbd59729",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('the', 8),\n",
       "             ('cat', 1),\n",
       "             ('sat', 1),\n",
       "             ('on', 1),\n",
       "             ('mat', 1),\n",
       "             ('dog', 2),\n",
       "             ('barked', 1),\n",
       "             ('loudly', 1),\n",
       "             ('at', 1),\n",
       "             ('night', 1),\n",
       "             ('i', 1),\n",
       "             ('love', 1),\n",
       "             ('eating', 1),\n",
       "             ('apples', 1),\n",
       "             ('and', 2),\n",
       "             ('bananas', 1),\n",
       "             ('she', 1),\n",
       "             ('is', 3),\n",
       "             ('reading', 1),\n",
       "             ('a', 3),\n",
       "             ('book', 1),\n",
       "             ('in', 1),\n",
       "             ('library', 1),\n",
       "             ('quick', 1),\n",
       "             ('brown', 1),\n",
       "             ('fox', 1),\n",
       "             ('jumps', 1),\n",
       "             ('over', 1),\n",
       "             ('lazy', 1),\n",
       "             ('machine', 1),\n",
       "             ('learning', 2),\n",
       "             ('branch', 1),\n",
       "             ('of', 2),\n",
       "             ('artificial', 1),\n",
       "             ('intelligence', 1),\n",
       "             ('deep', 1),\n",
       "             ('models', 1),\n",
       "             ('need', 1),\n",
       "             ('lot', 1),\n",
       "             ('data', 1),\n",
       "             ('weather', 1),\n",
       "             ('today', 1),\n",
       "             ('sunny', 1),\n",
       "             ('bright', 1),\n",
       "             ('natural', 1),\n",
       "             ('language', 1),\n",
       "             ('processing', 1),\n",
       "             ('helps', 1),\n",
       "             ('computers', 1),\n",
       "             ('understand', 1),\n",
       "             ('text', 1),\n",
       "             ('he', 1),\n",
       "             ('bought', 1),\n",
       "             ('fresh', 1),\n",
       "             ('vegetables', 1),\n",
       "             ('from', 1),\n",
       "             ('market', 1)])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2257d02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = tokenizer.texts_to_sequences(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "11ee5da7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2, 9, 10, 11, 2, 12],\n",
       " [2, 5, 13, 14, 15, 16],\n",
       " [17, 18, 19, 20, 6, 21],\n",
       " [22, 3, 23, 4, 24, 25, 2, 26],\n",
       " [2, 27, 28, 29, 30, 31, 2, 32, 5],\n",
       " [33, 7, 3, 4, 34, 8, 35, 36],\n",
       " [37, 7, 38, 39, 4, 40, 8, 41],\n",
       " [2, 42, 43, 3, 44, 6, 45],\n",
       " [46, 47, 48, 49, 50, 51, 52],\n",
       " [53, 54, 55, 56, 57, 2, 58]]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51327083",
   "metadata": {},
   "source": [
    "Add Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5423e56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "16c81faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = pad_sequences(sequences, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8d0c1cfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2,  9, 10, 11,  2, 12,  0,  0,  0],\n",
       "       [ 2,  5, 13, 14, 15, 16,  0,  0,  0],\n",
       "       [17, 18, 19, 20,  6, 21,  0,  0,  0],\n",
       "       [22,  3, 23,  4, 24, 25,  2, 26,  0],\n",
       "       [ 2, 27, 28, 29, 30, 31,  2, 32,  5],\n",
       "       [33,  7,  3,  4, 34,  8, 35, 36,  0],\n",
       "       [37,  7, 38, 39,  4, 40,  8, 41,  0],\n",
       "       [ 2, 42, 43,  3, 44,  6, 45,  0,  0],\n",
       "       [46, 47, 48, 49, 50, 51, 52,  0,  0],\n",
       "       [53, 54, 55, 56, 57,  2, 58,  0,  0]], dtype=int32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
